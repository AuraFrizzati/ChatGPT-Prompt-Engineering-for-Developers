{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSdR8iBfqM0c+Uq0ZjoiSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AuraFrizzati/ChatGPT-Prompt-Engineering-for-Developers/blob/main/00_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "The use of **API calls to Large Language Models (LLMs)** as a developer tool to quickly build software applications is still very underappreciated.\n",
        "\n",
        "\n",
        "There are two main types of LLMs:\n",
        "- **Base LLMs**: \n",
        "  - **Predict next word** based on **text training data**. They are often trained on large amounts of data from the internet and other sources to figure out what is the next most-likely word that will folow. \n",
        "  - Examples: \n",
        "    - <font color='red'>`Once upon a time there was a unicorn`</font>...`that lived in a magical forest with all her unicorn friends` \n",
        "    - <font color='red'>`What is the capital of France?`</font>...`What is France's largest city?` or `What is France's population` or `What is the currency of France?`. The completion is not very informative since it is based on what the model has found in the web (e.g. a quiz about France)\n",
        "- **Instruction-Tuned LLMs**:\n",
        "  - It tries to follow instructions: fine-tune on instructions and good attempts at following instructions\n",
        "  - Example:\n",
        "    - <font color='red'>`What is the capital of France?`</font>...`The capital of France is Paris`\n",
        "  - This kind of LLM, as well, has been **trained on a huge amount of text data** and then further **fine-tuned with inputs/outputs that are instructions and good attempts to follow those instructions**, and finally often further refined using a technique called **Reinforcement Learning from Human Feedback** (**RFLH**). RFLH makes the system better able to be helpful and follow instructions.\n",
        "  - These models are trained to be **helpful**, **honest** and **harmless**. They are less likely to output problematic  text (e.g. toxic output) in comparison to Base LLMs.\n",
        "  - For these improvements, a lot of the practical user scenarios have been shifting towards instruction-tuned LLMs.\n"
      ],
      "metadata": {
        "id": "m7XBAb1qJly2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This course will focused on **best-practices for instruction-tuned LLMs**."
      ],
      "metadata": {
        "id": "G3EOyuvZJsfR"
      }
    }
  ]
}